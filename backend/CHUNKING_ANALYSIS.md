# ğŸ“Š Analyse du Chunking & Reranking - SystÃ¨me RAG

## ğŸ” Ã‰tat Actuel du SystÃ¨me

### 1. **StratÃ©gie de Chunking** (config actuelle)

#### Chunking par Phrases (ACTIF) 
```python
chunking_strategy: "sentence"
sentences_per_chunk: 5      # 5 phrases par chunk
sentence_overlap: 1         # 1 phrase de chevauchement
```

**Comment Ã§a marche:**
- DÃ©coupe le texte en phrases individuelles avec regex
- Groupe 5 phrases ensemble â†’ 1 chunk
- Overlap de 1 phrase entre chunks consÃ©cutifs
- Ignore phrases < 10 caractÃ¨res

**Avantages:** âœ…
- Chunks sÃ©mantiquement cohÃ©rents
- Maintient le contexte de phrases complÃ¨tes
- Overlap naturel

**ProblÃ¨mes:** âŒ
- **Chunks trop courts** pour questions complexes (5 phrases = ~100-200 mots)
- Ne capture pas les sections entiÃ¨res (ex: Article + sous-articles)
- Pas de hiÃ©rarchie documentaire (titre â†’ paragraphe â†’ article)

---

#### Chunking par Tokens (DISPONIBLE mais dÃ©sactivÃ©)
```python
chunking_strategy: "token"  # DÃ©sactivÃ© actuellement
chunk_size: 800            # 800 tokens (~600 mots)
chunk_overlap: 200         # 200 tokens de chevauchement
```

**Comment Ã§a marche:**
- RecursiveCharacterTextSplitter de LangChain
- DÃ©coupe intelligente avec prioritÃ© aux sÃ©parateurs:
  1. `\n\n\n` (sections multiples)
  2. `\nARTICLE ` (dÃ©but d'article)
  3. `\nSection ` (dÃ©but de section)
  4. `\n\n` (paragraphes)
  5. `. ` (phrases)
  6. `, ` (virgules en dernier recours)

**Avantages:** âœ…
- **Chunks plus longs** â†’ plus de contexte
- Respect structure documentaire (articles, sections)
- Overlap de 25% (200/800)

**ProblÃ¨mes:** âŒ
- Actuellement **DÃ‰SACTIVÃ‰**

---

### 2. **Pipeline de Retrieval**

```
1. HyDE gÃ©nÃ¨re rÃ©ponse hypothÃ©tique (250 tokens)
   â†“
2. Embedding de la rÃ©ponse hypothÃ©tique
   â†“
3. Recherche vectorielle: TOP 40 chunks (initial_top_k=40)
   â†“
4. Reranking avec bge-reranker-v2-m3 (tous les 40)
   â†“
5. Filtrage par seuil: rerank_threshold=0.05
   â†“
6. DiversitÃ©: 1 chunk max par document si enforce_diversity=True
   â†“
7. Limite finale: top_k_results=12 chunks
```

---

### 3. **ProblÃ¨me IdentifiÃ© avec MREL/Buffer**

**Logs de ton test:**
```
ğŸ“Š Raw reranker scores - min: 0.000, max: 0.020
âš™ï¸  Rerank filter disabled â€” every reranked chunk is kept
ğŸ¯ Diversity applied: 1 documents used, distribution: {UUID(...): 3}
```

**Diagnostic:**
1. âŒ **Reranker scores catastrophiques**: max=0.020 (normalisÃ© 0-1)
   - Signifie: le reranker trouve les chunks **trÃ¨s peu pertinents**
   - Raison probable: chunks trop courts (5 phrases) ne contiennent pas assez de contexte

2. âŒ **1 seul document trouvÃ©** (CRD4.pdf)
   - DiversitÃ© enforcÃ©e mais un seul doc pertinent
   - Seulement 3 chunks retenus â†’ contexte insuffisant

3. âŒ **Pages trouvÃ©es peu pertinentes**: p.92, p.66 (table matiÃ¨res), p.12 (pÃ©rimÃ¨tre)
   - p.93 manquante (contient M-MDA dÃ©taillÃ©)
   - Chunks ne capturent pas la section complÃ¨te sur MREL/buffer interaction

---

## ğŸ’¡ Solutions d'AmÃ©lioration

### **Option A: Chunks plus longs (RECOMMANDÃ‰)** ğŸ†

**Action:** Passer au chunking par tokens avec chunks plus gros

```python
# Dans .env ou config.py
CHUNKING_STRATEGY=token
CHUNK_SIZE=1200           # â†‘ de 800 Ã  1200 tokens (~900 mots)
CHUNK_OVERLAP=300         # â†‘ de 200 Ã  300 (25% overlap)
```

**Avantages:**
- âœ… Plus de contexte par chunk (900 mots vs 100-200 actuellement)
- âœ… Capture sections complÃ¨tes (Article + sous-articles + explications)
- âœ… Meilleurs scores de reranking (plus de contenu pertinent)
- âœ… Respect structure documentaire (ARTICLE, Section, etc.)

**InconvÃ©nients:**
- âš ï¸ Chunks plus gros = plus de tokens envoyÃ©s au LLM
- âš ï¸ CoÃ»t lÃ©gÃ¨rement supÃ©rieur (mais offset par meilleure qualitÃ©)

---

### **Option B: Augmenter initial_top_k**

```python
INITIAL_TOP_K=60          # â†‘ de 40 Ã  60
TOP_K_RESULTS=15          # â†‘ de 12 Ã  15
```

**Avantages:**
- âœ… Plus de chunks candidats pour le reranking
- âœ… Meilleure chance de trouver p.93 M-MDA

**InconvÃ©nients:**
- âš ï¸ Reranking plus lent (60 chunks vs 40)
- âš ï¸ Ne rÃ©sout pas le problÃ¨me de chunks trop courts

---

### **Option C: Ajuster le seuil de reranking**

```python
RERANK_THRESHOLD=0.01     # â†“ de 0.05 Ã  0.01 (plus permissif)
```

**Avantages:**
- âœ… Garde plus de chunks aprÃ¨s reranking

**InconvÃ©nients:**
- âŒ Risque de garder des chunks non pertinents
- âŒ Bruit dans le contexte

---

### **Option D: Injection manuelle de sections critiques** ğŸ¯

**Action:** DÃ©tecter mots-clÃ©s et injecter sections spÃ©cifiques

```python
# Dans rag_service.py _augment_with_targeted_sections()
if "MREL" in query and ("buffer" in query or "CBR" in query):
    # Injecter chunks des pages 92-93 avec metadata section="MREL-Buffer"
    targeted_chunks = self._fetch_chunks_by_section("MREL", "buffer")
```

**Avantages:**
- âœ… Garantit que le contenu pertinent arrive au LLM
- âœ… Pas de dÃ©pendance au retrieval vectoriel

**InconvÃ©nients:**
- âš ï¸ Maintenance manuelle des mappings query â†’ sections
- âš ï¸ Pas scalable si beaucoup de topics

---

## ğŸ¯ Plan d'Action RecommandÃ©

### **Phase 1: AmÃ©liorer le chunking (PRIORITÃ‰ 1)** â­

1. **Activer chunking par tokens:**
   ```bash
   # Dans backend/.env
   CHUNKING_STRATEGY=token
   CHUNK_SIZE=1200
   CHUNK_OVERLAP=300
   ```

2. **Reprocesser les documents:**
   ```bash
   cd backend
   python3 scripts/reprocess_all_documents.py
   ```

3. **Tester avec la mÃªme query MREL/buffer:**
   - VÃ©rifier les reranker scores (devrait Ãªtre > 0.3)
   - VÃ©rifier les pages trouvÃ©es (devrait inclure p.93)

---

### **Phase 2: Optimiser le retrieval (PRIORITÃ‰ 2)** â­

Si Phase 1 ne suffit pas:

1. **Augmenter initial_top_k:**
   ```python
   INITIAL_TOP_K=60
   TOP_K_RESULTS=15
   ```

2. **Ajuster enforce_diversity:**
   ```python
   ENFORCE_DIVERSITY=False  # Autorise plusieurs chunks du mÃªme doc
   ```

---

### **Phase 3: Injection manuelle (PRIORITÃ‰ 3)** ğŸ”§

En dernier recours, si questions rÃ©currentes sur MREL/buffer:

1. Ajouter dÃ©tection de keywords dans `_augment_with_targeted_sections()`
2. Mapper query patterns â†’ sections prÃ©cises
3. Injecter avec similarity=1.0

---

## ğŸ“ˆ MÃ©triques Ã  Surveiller AprÃ¨s Changements

```
âœ… Reranker scores > 0.3 (actuellement 0.020)
âœ… Diversity: 1-3 documents (actuellement 1)
âœ… Pages trouvÃ©es: p.92, p.93, p.85 (actuellement p.92, p.66, p.12)
âœ… Citations dans rÃ©ponse: 6-10 <mark> tags (actuellement 2)
âœ… Longueur chunks: 800-1200 tokens (actuellement ~100-200 mots/5 phrases)
```

---

## ğŸ”„ Comparaison Chunking Strategies

| CritÃ¨re | Sentence (actuel) | Token (recommandÃ©) |
|---------|-------------------|-------------------|
| Taille chunk | 5 phrases (~100-200 mots) | 1200 tokens (~900 mots) |
| Overlap | 1 phrase | 300 tokens (25%) |
| Structure doc | âŒ IgnorÃ©e | âœ… RespectÃ©e (ARTICLE, Section) |
| Contexte | âš ï¸ LimitÃ© | âœ… Large |
| Reranker scores | âŒ 0.020 | âœ… Attendu >0.3 |
| Pertinence | âš ï¸ Fragments | âœ… Sections complÃ¨tes |
| CoÃ»t LLM | âœ… Bas | âš ï¸ Moyen (+30%) |
| QualitÃ© rÃ©ponse | âŒ GÃ©nÃ©rique | âœ… DÃ©taillÃ©e avec citations |

---

## ğŸš€ Commande Rapide pour Tester

```bash
# 1. Modifier .env
echo "CHUNKING_STRATEGY=token" >> backend/.env
echo "CHUNK_SIZE=1200" >> backend/.env
echo "CHUNK_OVERLAP=300" >> backend/.env

# 2. Reprocesser documents
cd backend
python3 scripts/reprocess_all_documents.py

# 3. Relancer backend
python3 run.py

# 4. Retester query MREL/buffer dans frontend
```
