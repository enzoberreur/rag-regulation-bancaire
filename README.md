# HexaBank Compliance Assistant

An AI-powered RAG (Retrieval-Augmented Generation) application that helps compliance officers at HexaBank interpret and analyze regulatory documents from ACPR, ECB, and EU AI Act.

## ğŸ¯ Overview

The HexaBank Compliance Assistant is a full-stack application that combines:
- **Document Processing**: Upload and process PDF, DOCX, and TXT regulatory documents
- **Vector Search**: Semantic search using BAAI/bge-m3 embeddings and PostgreSQL with pgvector
- **LLM Integration**: OpenAI GPT-4 for intelligent question answering with streaming responses
- **Citation System**: Automatic source citation for transparency and compliance validation

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React + Vite  â”‚ â”€â”€â”€â”€â”€â”€> â”‚  FastAPI + RAG  â”‚ â”€â”€â”€â”€â”€â”€> â”‚  PostgreSQL +   â”‚
â”‚    Frontend     â”‚  SSE    â”‚     Backend     â”‚         â”‚    pgvector     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                            â”‚
        â”‚                            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> OpenAI API
                                                           (GPT-4 + Embeddings)
```

## ğŸ› ï¸ Tech Stack

### Backend
- **Framework**: FastAPI (Python 3.11+)
- **LLM**: OpenAI GPT-4 with streaming support
- **Embeddings**: BAAI/bge-m3 (multilingual, 1024 dimensions)
- **Database**: PostgreSQL 15+ with pgvector extension
- **ORM**: SQLAlchemy 2.0
- **Document Processing**: PyMuPDF, python-docx, tiktoken
- **Async**: AsyncIO with OpenAI async client

### Frontend
- **Framework**: React 18 with TypeScript
- **Build Tool**: Vite
- **UI Components**: shadcn/ui (Radix UI primitives)
- **Styling**: Tailwind CSS
- **State Management**: React Hooks
- **Streaming**: Server-Sent Events (SSE)

### Database Schema
- **Documents**: Stores uploaded files metadata
- **DocumentChunks**: Text chunks with vector embeddings (1024-dim)
- **Indexes**: HNSW index for fast vector similarity search

## ğŸ“‹ Prerequisites

- **Node.js**: 18+ and npm
- **Python**: 3.11+
- **PostgreSQL**: 15+ with pgvector extension
- **OpenAI API Key**: For GPT-4 and embeddings

## ğŸš€ Quick Start

### 1. Clone and Setup Environment

```bash
git clone <your-repo>
cd LLMOPS-product
```

### 2. Database Setup

```bash
# Install PostgreSQL with pgvector
brew install postgresql@15
brew install pgvector  # macOS

# Create database
createdb hexabank_compliance

# Enable pgvector extension
psql hexabank_compliance -c "CREATE EXTENSION IF NOT EXISTS vector;"
```

### 3. Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp env.example .env
# Edit .env with your settings:
# - OPENAI_API_KEY=your_key
# - DATABASE_URL=postgresql://user:password@localhost:5432/hexabank_compliance

# Initialize database
python scripts/init_db.py

# Run backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### 4. Frontend Setup

```bash
# In a new terminal
cd /path/to/project

# Install dependencies
npm install

# Run frontend
npm run dev
```

### 5. Access the Application

- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs

### One-Command Launch

```bash
./run.sh  # Starts both backend and frontend
```

## ğŸ“ Project Structure

```
LLMOPS-product/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/              # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py       # Chat + streaming endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py  # Document upload/management
â”‚   â”‚   â”‚   â””â”€â”€ health.py     # Health check
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py     # Settings management
â”‚   â”‚   â”‚   â””â”€â”€ database.py   # Database connection
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ document.py   # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_service.py          # Main RAG logic
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding_service.py    # BAAI/bge-m3 embeddings
â”‚   â”‚   â”‚   â”œâ”€â”€ document_processor.py   # Document chunking
â”‚   â”‚   â”‚   â””â”€â”€ text_extractor.py       # PDF/DOCX extraction
â”‚   â”‚   â””â”€â”€ main.py           # FastAPI app
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ init_db.py        # Database initialization
â”‚   â”‚   â””â”€â”€ create_indexes.py # Vector index creation
â”‚   â”œâ”€â”€ storage/              # Uploaded documents
â”‚   â””â”€â”€ pyproject.toml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx    # Main chat UI
â”‚   â”‚   â”œâ”€â”€ ChatMessage.tsx      # Message rendering with citations
â”‚   â”‚   â”œâ”€â”€ DocumentUpload.tsx   # Upload interface
â”‚   â”‚   â””â”€â”€ ObservabilityPanel.tsx  # Metrics display
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ api.ts            # API client
â”‚   â””â”€â”€ App.tsx
â”œâ”€â”€ README.md
â””â”€â”€ run.sh                    # Launch script
```

## ğŸ”‘ Key Features

### 1. Document Processing Pipeline
```
Upload â†’ Text Extraction â†’ Chunking (500 tokens) â†’ Embedding â†’ Vector Storage
```

### 2. RAG Query Flow
```
User Query â†’ Embedding â†’ Vector Search (top 5) â†’ Context Building â†’ LLM â†’ Streaming Response
```

### 3. Response Formatting
The system uses intelligent regex-based post-processing to ensure:
- âœ… Blank lines before numbered lists (1., 2., 3.)
- âœ… Proper spacing for section headers
- âœ… Protected decimal numbers (2.5%) and dates (December 31, 2025)
- âœ… Line breaks between sentences
- âœ… Preserved bullet points formatting

### 4. Streaming Architecture
- **Backend**: Collects full LLM response â†’ Normalizes formatting â†’ Encodes newlines â†’ Streams via SSE
- **Frontend**: Accumulates chunks â†’ Decodes newlines â†’ Renders with proper formatting

### 5. Cost & Performance Tracking
Real-time metrics for each query:
- Token usage (input/output)
- API cost calculation
- Similarity scores
- Latency measurement

## ğŸ§ª API Endpoints

### Health Check
```bash
GET /api/health
```

### Document Management
```bash
POST   /api/documents/upload          # Upload document
GET    /api/documents/                # List all documents
GET    /api/documents/{id}/view       # View document
DELETE /api/documents/{id}            # Delete document
```

### Chat
```bash
POST /api/chat/stream                 # Streaming chat (SSE)
POST /api/chat                        # Non-streaming chat
```

### Example: Streaming Chat Request
```bash
curl -X POST http://localhost:8000/api/chat/stream \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What are the key requirements of ACPR Regulation 2024-15?",
    "history": []
  }'
```

## ğŸ”§ Configuration

### Backend (.env)
```bash
# OpenAI
OPENAI_API_KEY=sk-...
LLM_MODEL=gpt-4
EMBEDDING_MODEL=text-embedding-3-large

# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/hexabank_compliance

# RAG Settings
TOP_K_RESULTS=5
CHUNK_SIZE=500
CHUNK_OVERLAP=50

# Storage
STORAGE_PATH=./storage/documents
```

### Frontend (vite.config.ts)
```typescript
export default defineConfig({
  server: {
    port: 3000,
    proxy: {
      '/api': 'http://localhost:8000'
    }
  }
})
```

## ğŸ› Troubleshooting

### Port Already in Use
```bash
# Kill processes on ports 3000 and 8000
lsof -ti:3000 | xargs kill -9
lsof -ti:8000 | xargs kill -9
```

### CORS Issues
Make sure `localhost:3000` and `localhost:3001` are in the CORS allowed origins in `backend/app/main.py`.

### Database Connection Failed
```bash
# Check PostgreSQL is running
brew services list

# Restart if needed
brew services restart postgresql@15
```

### Embedding Model Loading Slow
First run downloads ~2GB model from Hugging Face. Subsequent runs use cached model.

## ğŸ“Š Performance Considerations

- **Embedding**: ~200ms per query (BAAI/bge-m3 on CPU)
- **Vector Search**: <50ms with HNSW index (5k chunks)
- **LLM Response**: 2-5s streaming (depends on context size)
- **Total Latency**: ~3-6s for typical query

### Optimization Tips
1. Use GPU for embeddings (3-5x faster)
2. Adjust `CHUNK_SIZE` based on document type
3. Tune `TOP_K_RESULTS` for accuracy vs speed
4. Use smaller embedding model for faster inference

## ğŸ”’ Security Notes

- API keys stored in `.env` (never commit!)
- CORS configured for development (restrict in production)
- SQL injection protected by SQLAlchemy ORM
- File uploads validated by extension
- No authentication implemented (add for production)

## ğŸ“ˆ Future Enhancements

- [ ] Multi-user authentication (JWT)
- [ ] Document version control
- [ ] Advanced filtering (date, document type)
- [ ] Export chat history
- [ ] Fine-tuned embedding model
- [ ] Caching layer (Redis)
- [ ] Kubernetes deployment
- [ ] Monitoring with Prometheus/Grafana

## ğŸ¤ Contributing

This is a student project for Albert School. Contributions welcome!

## ğŸ“„ License

Educational project - Albert School Year 2

## ğŸ™‹ Support

For issues or questions, please contact the development team.

---

**Built with â¤ï¸ at Albert School**